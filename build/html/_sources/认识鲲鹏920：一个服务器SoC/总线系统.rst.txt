.. Copyright by Kenneth Lee. 2020. All Right Reserved.

总线系统
========

概述
----
鲲鹏使用一条全局的Cache Coherent的总线把所有的设备连在一起，使CPU和设备都可以访
问系统中的任何内存。

        | Cache Coherent
        | 由于跨越总线访问其他数据需要时间，肯定不如直接在本子系统中直接访问的
        | 速度快，设计者会为本子系统设计本地的缓冲，这个缓冲称为Cache。Cache带
        | 来了性能的提升，但同时也带来对数据的多副本问题。部分总线的设计，需要
        | 每个子系统自己处理这种多副本问题。而Cache Coherent的总线本身带了协议
        | 解决这样的同步问题，可以保证如果一个副本给更新了，其他副本在被读取前
        | 总是被更新了的。

计算机的总线互联技术称为NoC，Network on Chip。是一个独立的技术门类，其中包括介
质，拓扑，安全，算法等各方面专门的研究。每种计算会根据自己的需求特点，选择不同
的技术方案和参数。

鲲鹏920使用以环状总线为基础的总线技术，下面是一个示意图：

.. figure:: kunpeng_ring_bus.svg
        :alt: Kungpeng Ring Bus

        鲲鹏环状总线示意图

环状总线结构有点像城市里的环线地铁：列车在一个环线上一直循环，在每个车站停车上
下乘客，无限循环。环状总线在时钟的驱动下，每个时钟周期，电路逻辑把数据从一个
Station发送到下一个Station。这种每次发送的数据称为Flit，多个Flit组成一个Packet，
Station向Packet加入自己的Flit或者读走发给自己的Flit。有了这样一个基础，总线就可
以在时钟的驱动上，一直循环把数据发给每个Station。

不同的应用会选择不同的总线参数，比如鲲鹏920的Flit为了保证Latency，会选择70%的空
载率，而使用相同技术的Hi1980（市场上所知的达芬奇AI芯片），首先保证吞吐率，它的
Flit利用率就会高达90%。

环状总线是Super Cluster内部的机制，如果需要跨越Super Cluster，就需要在部分
Station上实现数据的“转乘”，从另一条线路出去。下面是一个鲲鹏920 4 SoC互联的示意
图：

todo：需要画一副4P的互联示意图，特别需要标识一下内存控制器的位置。

通过这样一个结构，无论是同一个芯片内部还是多个芯片互联，系统里的任意两个单元都
是可以互相访问的。

对于鲲鹏，由于同一个封装之内的线路是可控的，所以跨Super Cluster的总线是不经过
Serdes电路的，而跨封装的，就需要经过这样一个过程了。所以，从通讯效率上看，Super
Cluster内是最快的，跨封装相对来说是最慢的，但作为总线，它仍比读写内存本身快得多
。

下面是一个我们在鲲鹏920上访问不同区域内存的速度测试：

todo：需要一个内存测试数据。

        | Serdes
        | Serdes是Serializer/Deserializer的缩写。现代计算机互联技术向高速发展后
        | 并行总线由于信号干扰的问题，很难在芯片之外的布线中实现，所以，在实际
        | 在进行芯片之外的互联中，都会使用串行通讯电路，这样就需要专门的技术实现
        | 这种串行和反串行的过程，这种方法，实践和具体的技术，称为Serdes。

但我们也看到了，其实数据转到系统不同位置上的时间是不同的。现代片上网络设计有很
多新的发展，比如通过3D的互联结构，提供更多样的路由优化，或者改变Flit的调度和流
控算法减少冲突等。但如果考虑到越来越多的节点和前面提到的引线的问题。我们几乎可
以预期未来更多核的系统，必然是NUMA系统。程序员是肯定需要关心到被访问的地址和本
设备的距离的。作为程序员的读者可能可以把这个作为未来开发的一个基本考虑。

        | NUMA
        | Non-uniform memory access. 这是一种计算机内存构架，不同位置的内存对
        | 不同的CPU的访问速度是不同的。感知NUMA的软件可以通过使用靠近本CPU的内
        | 存，从而让性能达到最优。

总线系统是鲲鹏所有全局设计的中心，CPU和设备访问内存，CPU访问设备，设备之间互相
访问，乃至中断的分发，都基于这个总线系统。后面我们会看到更多和它相关的设计。

MMIO和物理地址编址
------------------
早期的计算机都把和内存的通讯，以及和设备的通讯分开。由于访问效率和访问功能不同
访问内存和进行设备通讯用的总线也不同。但随着技术的发展，这种分离已经逐渐没有必
要了。

        | MMIO
        | Memory Mapped Input/Ouput。
        | MMIO是为了区分把内存和IO访问分离的技术而建立的概念。
        | 它表示内存和IO是在一个地址空间中编址的一种总线地址应用方式。

鲲鹏920就使用一条系统总线沟通所有的设备和内存控制器。所以它访问内存的某个位置和
访问某个设备的方法从指令上看起来都是一样的。无论设备还是CPU发出地址请求，经过
MMU或者SMMU翻译，最后都会成为Flit中的一个请求，这个请求中包含读写类型，访问字长
，安全标记，写入数据等等信息，从一个Station发出去，这个Station会根据目标地址得
到目标的Station ID，Flit到达目的地Station，就会从那里读走，然后控制目标一侧的设
备做出反应。无论是读写，也无论消息的目标是内存控制器还是设备，最后都是消息。

当然，在这个消息之上的协议行为可能是不同的，比如写操作可能就不需要等待目标设备
响应，读操作就需要等待。读写内存可以基于Cacheline一次读写整个Cacheline，读设备
这需要严格根据字长的要求进行控制。如果内存配置了交织访问，还需要对请求进行拆解
，变成多个针对不同目标的请求。鲲鹏通过一个实现在每个Station中的称为DAW（Danymic
Address Window）的设计对地址的范围进行判断，再根据这个判断决定最终的Flit的内容。
DAW在BIOS配置阶段由BIOS全局分配给每个Station，从而让每个Station的用户有全局唯一
的地址空间。

鲲鹏920使用48位物理地址，其中x位（todo：4位？要确认一下）用于DAW窗口，剩下的是
Station内部使用的偏移地址，这整个地址都在Flit内，收到的设备如何处理它，就是地址
读写协议的问题了。

从编程的角度，我们可以从两方面来控制CPU或者设备发出的地址请求。其一是发出的地址
的指令指定的字长。鲲鹏遵循ARMv8标准，ARMv8对不同的字长有不同的访问指令，比如：

* 8位读写 strb, ldrb
* 16位读写 strh, ldrh
* 32/64位读写 str, ldr （取决于操作数的不同会生成不同的指令编码）
* 128位读写 stp, ldp

这些指令只是CPU的期望，当这些请求发到MMU上，MMU要根据地址对其进行解释，再变成物
理地址。下面是一种ARMv8的页表的格式：

        .. figure:: armv8_4k_pte_format.svg

其中的MemAttr指明了这个目标地址是内存类型的还是设备类型的，内存类型分三种：

* MemAttr[3:2]=01 Inner或者Outer NonCacheable
* MemAttr[3:2]=10 Outer Write-through Cacheable
* MemAttr[3:2]=11 Outer Write-back Cacheable

Inner和Outter基本上是两种Cache Coherency的界限，Inner之内内存是CC的，Outter则需
要任用进行Cache同步。鲲鹏920上，对所有计算子系统和内存，都是Inner域中的内存。

Write-through和Write-back，是两种常见的Cache写入算法，表示写入Cache的时候是否理
解更新内存。

Inner的内存又分三种：

* MemAttr[1:0]=01 NonCacheable
* MemAttr[1:0]=10 Write-through Cacheable
* MemAttr[1:0]=11 Write-back Cacheable

设备（MemAttr[3:2]=00）分四种类型：

* MemAttr[1:0]=00 nGnRnE
* MemAttr[1:0]=01 nGnRE
* MemAttr[1:0]=10 nGRE
* MemAttr[1:0]=11 GRE

这里定义了三种访问行为定义：

* G，Gatthering，这表示是否允许MMU和总线收集多个请求以后一次发出去
* R，Reordering，这个表示是否允许MMU和总线对同一个通讯目标重排请求的顺序
* E，Write Acknowledgement，这个表示如何认可一个写操作成功了。E表示等写响应消息
  回来才是写成功了，nE表示只要发出去就成功了。

todo：鲲鹏在实现G、R、E的时候的特殊考虑。

在5.5主线的Linux Kernel的实现中，我们可以看到它默认选择的属性：

这是设备的映射：
.. code-block:: c
        //arch/arm64/include/asm/io.h
        #define ioremap(addr, size)		__ioremap((addr), (size), __pgprot(PROT_DEVICE_nGnRE))
        #define ioremap_wc(addr, size)		__ioremap((addr), (size), __pgprot(PROT_NORMAL_NC))

这是内存的映射：
.. code-block::
        // arch/arm64/mm/proc.S
	ldr	x5, =MAIR(0x00, MT_DEVICE_nGnRnE) | \
		     MAIR(0x04, MT_DEVICE_nGnRE) | \
		     MAIR(0x0c, MT_DEVICE_GRE) | \
		     MAIR(0x44, MT_NORMAL_NC) | \
		     MAIR(0xff, MT_NORMAL) | \
		     MAIR(0xbb, MT_NORMAL_WT)
	msr	mair_el1, x5

.. code-block:: c
        // arch/arm64/include/asm/pgtable-prot.h
        #define PROT_NORMAL (PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_WRITE | PTE_ATTRINDX(MT_NORMAL))


这个地方用的页表格式和我们前面举的例子用的那个不太一样，它的内存属性不是直接放
在页表项中的，而是页表项放一个索引，内容放在寄存器MAIR中，这里MT_NORMAL是5，所
以其实索引了NORMAL_WT的配置，但最终逻辑都是一样的。

ARMv8支持双Stage页表翻译，每个Stage支持多种页表格式，每种页表还支持多种页的大小
。所以这里其实有很多的变体，但其实我们不是那么关心这些实现细节，我们主要还是希
望通过这个实例，让读者对于访存的语义映射为一种硬件的行为的时候，大致会有哪些方
面的细节问题需要被考量有所了解。

综合起来，CPU发出一个总线地址请求，这个请求会有自己的要求，但MMU和DAW也会对这个
请求做出自己的理解，并按这个理解对这个请求进行二次解释，最终变成对设备（包括DDR
控制器）的请求。

这也说明了按抽象层次的语义编程的重要性。在这种合作和发展的过程中，每个抽象层次
只是维持自己保证的语义，在新的版本中可能会在没有承诺的部分做出改变，如果工程是
按“我试过了，这样可以”的心态来写程序，可能任何一个环节升级，这个程序就不能工作
了。

有些设备是对访问的长度和方法是有明确要求的，比如下面是鲲鹏920的加速器设备设置邮
箱命令的方法，它强行使用128位的访问，如果分开成两次64位访问，就会出错：

.. code-block:: c
        // drivers/crypto/hisilicon/qm.c
        static void qm_mb_write(struct hisi_qm *qm, const void *src)
        {
                void __iomem *fun_base = qm->io_base + QM_MB_CMD_SEND_BASE;
                unsigned long tmp0 = 0, tmp1 = 0;

                if (!IS_ENABLED(CONFIG_ARM64)) {
                        memcpy_toio(fun_base, src, 16);
                        wmb();
                        return;
                }

                asm volatile("ldp %0, %1, %3\n"
                             "stp %0, %1, %2\n"
                             "dsb sy\n"
                             : "=&r" (tmp0),
                               "=&r" (tmp1),
                               "+Q" (*((char __iomem *)fun_base))
                             : "Q" (*((char *)src))
                             : "memory");
        }

Cache
-----
Cache互联设计的典型优化手段。它基于两个简单的想法：

1. 如果一个访问对象很远，而我又没有确定最终的结果，我可以先用更近的对象暂存数据
   ，等确切决定这个最终结果了，再一次更新到那个对象上。

2. 如果一个访问对象很慢，而我又没有确定最终的结果，我可以先用更快的对象暂存数据
   ，等确切决定这个最终结果了，再一次更新到那个对象上。

这个想法的前提是这个事实存在：“我还没有确定最终的结果”，这个事实不是总是存在的，
比如做IO的时候：我在内存中有一个数据包，我需要发到设备上，发完我就要发新的数据
了，这时做Cache就是多余的。但如果我要对这个数据包做计算，对每一段都做一个
Checksum，然后还要根据里面的域进行查表，然后更新他们的内容，这个结果没有完成之
前，这些数据除了本CPU任何其他总线上的设备都不关心，这时，使用Cache就有必要了。

下面是鲲鹏920的内存三级Cache设计示意图：

todo：晚点画这个图。

todo：介绍L3 tag和data分离的设计。

不同级别Cache的访问速度是差别是很大的，下面是鲲鹏920不同级别Cache访问速度的参考
数据：

todo：Cache访问数据参考数据。

Cache可以设计成对程序员透明，也可以设计成不透明。所谓透明，就是程序员可以不认为
存在Cache，认为自己访问的就是内存，所有因为Cache可能造成的误会，都由硬件想办法
弥补。而不透明就是程序员知道自己正在使用的是Cache还是实际的内存，主动维护两种内
存的关系。现代服务器基本都使用透明设计，但这些透明大部分时候只是对功能透明，对
性能并不透明，所以进行性能优化的时候，常常还是要考虑到Cache存在的影响。

Cacheline
```````````
讨论Cache不能回避的一个问题是Cacheline，我们理解一下为什么会存在Cacheline。所有
的暂存表，都存在一个比原表小的问题，然后会会存在一个地址离散的问题。原表可以是
连续地址，每个地址都有内容。而暂存表不行，你的每个空间都会需要一个地址说明它是
原表的哪个位置。对于内存来说，这个成本尤其高，因为一个地址和地址的内容基本上信
息量是一样的。所以，从逻辑上说，无论我们用什么算法来解决这个问题，一个地址代表
一个足够长的内容是必然的。这个足够长的内容，就是一个Cacheline。

而为了效率，如果针对的是内存，大部分访问者，都会以Cacheline为单位来获取内存，即
使你访问的仅仅是Cachenline中的一个字节。这样，把数据结构按Cacheline的长度对齐，
就会有访问上的优势。

todo：多级Cache的Cacheline长度不一样的问题如何考量？

在编程上，我们知道实现是一回事，但程序员还是应该遵循语义来编程，一般程序最好不
要感知Cacheline的长度，只有在性能强相关的关键程序中，才适合去获取本平台的
Cacheline长度。对于Linux平台，大部分时候我们可以通过getconf命令或者sysconf系统
调用获得这些参数。

todo：鲲鹏920的平台上如何知道Cachenline的长度？

Cache Prefetch
```````````````
Cache Prefetch也是一个针对Cache的优化设计。Cache比实际的内存快很多，所以如果我
们可以提前加载部分内存到Cache中，就会在性能上有优势。

鲲鹏920支持ARMv8的prfm指令，这个指令……todo

Cache Coherency
================
todo
