.. Copyright by Kenneth Lee. 2020. All Right Reserved.

综述
====

存储提供服务器的数据持久化能力。对于一台服务器本身，它可能需要这样一些存储能力
：

* 支持服务器本地运行需要的存储
* 服务器为其他业务提供存储服务需要的存储
* 服务器访问远程存储的能力

支持服务器本地运行需要的存储
----------------------------

NOR Flash
~~~~~~~~~
todo

Nand Flash
~~~~~~~~~~~
todo

大容量存储介质
~~~~~~~~~~~~~~
todo

SAS和SATA接口
~~~~~~~~~~~~~
todo

服务器为其他业务提供存储服务需要的存储
--------------------------------------
todo：iSCSI等

服务器访问远程存储的能力
------------------------
todo

软件协议栈
-----------
本章我们仍使用Linux发行版作为我们讨论软件方案的基本对象。不过虽然现在很多数据
中心使用Linux作为基础的OS解决方案，但对于存储业务本身，大部分使用的是其他闭源
的解决方案，我们用Linux作为讨论问题的基础，读者可能需要在这个基础之上更关注它
可以形成的变体。

存储关心的问题是数据持久化，有一个有趣的问题是，如果DRAM在断电以后仍可以存在，
我们还需要存储功能吗？其实是需要的，这有几个原因：

* 一个进程或者一个VM等可以独立管理实体的状态，不但和它的所有数据相关，还和
  它的环境相关。断电以后，这个外部环境会发生变化，原先的数据并不是都能直接
  使用，所以，对一个程序来说，什么数据是运行状态的，什么数据是静态的，其实是需
  要区分的。即使DRAM的内存不会消失，程序还是会去感知自己的一个“存储”过程。

* 多个应用可能会通过静态存储共享数据，这种共享需要一种独立的地址空间，而不是内
  存的地址空间。

所以，文件几乎是所有存储都需要的基本形式，无论对于非常传统的慢速磁带设备，还是
对于今天的高速非易失内存。如果速度足够快，我们只是需要把文件映射为内存，然后当
内存使用，如果速度不够快，就把写入和读出的过程以块为单位从内存映射到慢速空间。

所以，Linux的存储子系统基本上表现为四层：

* VFS层，负责提供文件接口
* Page Cache层，为VFS对BIO层提供缓冲服务
* BIO层，负责把文件的块，映射为设备的块
* 介质层，负责最终访问物理介质。
  

        .. figure:: linux_storage_subsystem.svg

这样分层只是构成一个基本的框架，它有很多不严谨的地方。比如VFS并非是存储模块专
用的，它还用于其他IO。当整个Linux系统作为存储的目标的时候，它也不经过VFS……但正
如本书一开始就提到的，整个IT产品就是不断进行抽象，替换和飞线的过程，每个层次都
可以被替换和越过。比如非易失存储可以跳过Page Cache，BIO层之上可以LVM或者DM这样
的转化层对介质进行分散和集中，而介质层又可以架在网络层之上聚合多个其他网络节点
提供BIO服务……这些变动背后，都是软硬件参数和市场需求变化在驱动着，很难进行提前
预判，工程师都是根据当下的参数和自己面对的市场，利用已经存在的软件框架进行调整
，很难说这些会有多长远。但也没有人能一下颠覆整个框架，因为软件的工作量摆在这里
，这样给了我们一个机会：抓住一个主干，然后具体分析一些变体，我们就能对这个系统
有一个基本的认识，到要解决新的问题的时候，我们再去研究具体的细节就好了。

VFS
~~~

如前所述，VFS层包含了各种内核访问的接口，很多功能都会封装为文件访问，但我们这
里只考虑它对存储业务的支持。

VFS把存储设备看做是一个连续或者非连续的线性空间，对这个线性空间赋予格式，从而
构成一个“文件系统”，文件系统的格式由文件系统类型（file_system_type）定义。文件
系统格式通过mount系统调用关联到一个块设备（block_device），从块设备的非易失存
储空间中读出文件系统的全局控制数据（称为super block，简称sb），最后基于这个sb
，找到所有其他数据的位置。

这个结构制造了另一个变体，sb也可以通过网络，多个设备，其他文件系统的文件中获得
，这样sb不一定需要关联到一个块设备（虽然sb中确实存在专门用于块设备的数据结构）
，只要你能基于sb响应所有的文件系统请求就能实现一样的功能。

所以，从存储上，我们可以认为VFS提供了一个访问文件系统的Client，而这个Client访
问什么地方的静态存储，这是可以任意定义的。但VFS不是唯一的访问Client，部分文件
系统，比如Hadoop用的HDFS就设计了自己的访问Client，而不使用VFS，这同样可以。

从Linux命令行的层面理解这个问题，我们挂载一个文件系统的命令是类似这样的：::

        mount -t ext4 /dev/sda1 /mnt/home

这里的ext4就是file_system_type的名字，/dev/sda1是参数，而/mnt/home是挂载在整个
系统的什么入口位置上。正如我们前面说的，文件系统不需要一定关联块设备，所以我们
这里说/dev/sda1是参数，而不是一个块设备，所以，你还会看到这样的命令：::

        mount -t proc none /proc
        mount -t sysfs howareyou /sys
        mount -t 9p -o trans=virtio my_9p_name /mnt
        mount -t nfs myhost:/export/home /mnt/home

这里面，proc和sys可以不需要参数，因为系统就这么一个proc文件系统，所以你可以写
none或者所有你系统的字符串。而这里的9p文件系统需要的virtio指定的字符串，而nfs
需要的是服务器的地址，这些参数都用作mount的主参数。

回到内核实现，下面是file_system_type涉及的回调：

        .. code-block:: c

	int (*init_fs_context)(struct fs_context *);
	struct dentry *(*mount) (struct file_system_type *, int, const char *, void *);
	void (*kill_sb) (struct super_block *);

这些回调的核心概念的这个fs_context，它表示file_system_type和一个存储介质关联时
的上下文，用户可以自己初始化这个上下文，这时实现上面的init_fs_context，这需要
用户自己提供，如果用户不需要控制这么多东西，可以改为实现mount，这时VFS会用默认
的init_fs_context（legacy_fs_context_ops），其中的get_tree回调需要使用你的
mount，文件系统类型在mount中负责基于默认的fs_context内容（参数也在其中了），找
到对应的参数，利用这些参数创建root目录的dentry和sb，为这些目录结构创建响应的回
调，这样整个目录递归结构就建立了。

总结起来，VFS重点就是维护整个目录结构，让file_system_type建立目录结构和不同非
易失存储的关联。

Page Cache层
~~~~~~~~~~~~~~

file_system_type从慢速的存储中读入数据，总需要内存作为缓存，这个部分由统一的模
块提供，这个层级，就是Page Cache层。提供Page Cache管理的模块称为filemap，但并
非每种file_system_type都使用这种服务，这也不一定表示这种file_system_type就一定
没有页缓冲，它完全可以自己申请页表来完成一样的功能。

filemap维护一个称为address_space的数据接口，它通过一个稀疏表管理整个文件线性空
间分段加载到内存中的数据。

我们在命令行运行free命令，会有这样的结果：::

        >free
                      total        used        free      shared  buff/cache   available
        Mem:       16135012     8784784     2000484     1198856     5349744     5759724
        Swap:      15999996       18688    15981308

其中的buff/cache就是这里提到的address_space管理的数据。其中buff（Buffers）是块
设备自己的inode对应的address_space，如果你直接访问块设备或者file_system_type访
问磁盘上的matedata，就会占据这部分空间（todo：这需要确认一下）。如果你直接访问
具体的文件，对应文件inode上的空间就是cache占据的页面空间。

todo：后面的内容待续

.. vim: fo+=mM tw=78
