.. Copyright by Kenneth Lee. 2020. All Right Reserved.

设备和设备总线
==============

前面介绍总线的时候我们介绍了鲲鹏的总线结构，本章我们看看设备是怎么连到总线上的。

忽略一些非关键功能的辅助设备，基于鲲鹏920构成的系统主要有两种类型的设备，一种内
置在SoC内，直接连接在NoC上。另一种是片外的，这些设备主要通过PCIe总线接入系统。
还有一些非高速设备使用其他一些特殊手段连接的，那些不是设计的重点，我们先忽略，
等需要介绍特定的设计的时候再讨论。

为了软件编程的方便，鲲鹏920的片内设备也提供PCIe一样的编程接口，所以，要理解这些
设备的设计，我们也需要先了解PCIe的工作原理。

PCIe
-----

PCIe从PCI接口发展而来，它向前兼容PCI的软硬件接口，旧的PCI接口卡可以插到PCIe的硬
件插槽中，旧的PCI驱动也可以直接驱动这些设备。这一方面带来了生态上的稳定，但另一
方面也带来很多不必要的负担。

正如我们前面提到的，PCIe对于PCI的一大升级是从并行总线换成了基于Serdes的多Lane
P2P串行总线。两者的区别图示如下：

.. figure:: pci_vs_pcie.svg

所以PCIe总线和鲲鹏的系统总线不同，它是一个树状的分层架构：

.. figure:: pcie_arch.svg

其中的RC是一个虚拟的概念，相当于软件对于PCI设备访问的一个抽象，实现相关，之后是
链接出去的独立的终点（EP，End Point）或者网桥，网桥复制这个结构，就形成整个网络
了。

这样的网络结构在理解上反而相对简单，不会像PCI总线那样，每个高层语义都要转化为物
理信号。PCIe的高层语义直接可以建立在通讯语义的基础上。这个协议上分成三层：

* 会话层：定义总线不同业务，比如内存/IO访问，中断，低功耗处理等的通讯和消息协议

* 数据链路层：定义会话，链路，通道等协议

* 物理层：定义Serdes层的协议要求

用户能感知的是会话层，直接决定功能。数据链路层，对使用者的感知呈现为对设备QoS的
控制，通过配置总线属性来访问。而物理层，基本上对用户不可见。

正因为这样一个架构，鲲鹏920的片内设备很容易就封装为会话层接口，对软件提供PCIe的
设备语义。有些协议，比如CCIX，可以工作在PCIe的数据链路层上，替换了会话层，只要
修改一下PCIe控制器的工作模式，就可以让部分端口工作在CCIX模式上了。


地址空间
````````
前面说过，PCI/PCIe的最大特点是提供了系统总线一样的语义，可以直接通过地址访问直
接访问每个设备。

但对于PCIe总线，这个问题比一般的系统总线设备会复杂得多。系统总线是固定的，有多
少设备一开始就可以知道，配合DAW机制，我们可以给每个设备固定的地址空间。而对于
PCIe的网桥，我们可以给它一个地址空间。但当数据发到这个网桥本身的时候，它怎么知
道这个数据属于下属的哪个设备或者哪个网桥呢？

这涉及PCIe总线控制器的设备发现过程（又称为枚举过程）。其原理是系统软件，比如
Boot Loader或者操作系统，在正式使用某个网桥下面的设备前，先通过某个固定的形式发
现下面有什么设备，发现设备后从自己的地址空间上给它分配地址空间，并告知每个网络
节点这个分配的空间范围，这样数据就可以根据地址的范围，经过层层转发，到达它需要
达到的目的地上了。

也就是说，系统给根桥分配了一段物理地址，根桥要配合软件把这段物理地址分配给下属
的设备和子网桥。PCIe把这个地址空间分成三部分：

* 配置空间。这在PCI标准中称为配置空间，在PCIe中作了扩展，称为ECAM空间，Enhanced
  Configuration Access echanism。在本文中，在不会引起误会的情况下，我们混用这两
  个概念，两者是兼容的。

* 内存地址空间，这个访问的可能是内存，也可能是MMIO。

* IO地址，这个是为了兼容传统有独立IO访问指令的处理器设计的。

我们补充解释一下最后这个IO地址的含义：前面我们说过，早期的CPU常常用不同的指令访
问内存和IO。新的处理器就只用MMIO，内存和IO是统一编址的。但PCIe兼容PCI，还支持传
统的IO地址。所以这里的三个地址其实有三种地址方式，其中配置地址可以映射为IO空间
的地址，或者MMIO空间的地址，在鲲鹏中，主要就是MMIO的地址。而内存地址主要就是指
总线的物理地址。而IO地址，需要CPU中用io指令发出这样的请求。在鲲鹏中，会把这个地
址空间映射为一段MMIO的空间。

下面是使用鲲鹏920的泰山服务器Linux 5.0内核启动时打印的IO空间对内存空间的映射：::

        Remapped I/O 0x00000000efff0000 to [io  0x0000-0xffff window]
        Remapped I/O 0x00000000ffff0000 to [io  0x10000-0x1ffff window]
        
对应的代码在drivers/acpi/pci_root.c:acpi_pci_root_remap_iospace()中。它从ACPI的
配置表中得到对应的物理空间，映射到内核虚拟空间中，以便在设备要求用IO的方式访问
的时候可以通过MMIO访问这个虚拟地址空间实现对应的IO请求。

除了IO空间，剩下的空间分配给配置地址空间和内存地址空间。前者对于PCIe层可见，后
者分配给每个设备和网桥后，用于设备和网桥本身的功能。

配置空间的地址用一个公共的首地址加上设备标识共同组成。PCI/PCIe用
Bus-Device-Function三个标识唯一表示网络中的一个设备。这个组合简称BDF，一般表示
为xx:yy.z的形式，其中xx是Bus ID，yy和z分别是Device和Function ID。三者的含义分别
是：

* Bus：总线编号，用于唯一标识系统中的一条总线

* Device：EP的编号，用于标识一个网桥上的一个设备

* Function：EP内一个独立的功能的编号，这用于一个硬件有多个功能的情形，比如一张
  同时支持Ethernet和RoCE的网卡，就包含两个Function了。

这样，如果我们知道一个配置空间的首地址和某个设备的BDF，我们就可以访问这个设备了
。某个BDF的设备的配置空间地址可以表示如下：

.. figure:: ecam_address.svg

下面是使用鲲鹏920的泰山服务器Linux 5.0内核启动时打印的所有ECAM空间：::

         acpi PNP0A08:00: ECAM at [mem 0xd0000000-0xd3ffffff] for [bus 00-3f]
         acpi PNP0A08:01: ECAM at [mem 0xd7b00000-0xd7bfffff] for [bus 7b]
         acpi PNP0A08:02: ECAM at [mem 0xd7a00000-0xd7afffff] for [bus 7a]
         acpi PNP0A08:03: ECAM at [mem 0xd7800000-0xd79fffff] for [bus 78-79]
         acpi PNP0A08:04: ECAM at [mem 0xd7c00000-0xd7dfffff] for [bus 7c-7d]
         acpi PNP0A08:05: ECAM at [mem 0xd7400000-0xd76fffff] for [bus 74-76]
         acpi PNP0A08:06: ECAM at [mem 0xd8000000-0xd9ffffff] for [bus 80-9f]
         acpi PNP0A08:07: ECAM at [mem 0xdbb00000-0xdbbfffff] for [bus bb]
         acpi PNP0A08:08: ECAM at [mem 0xdba00000-0xdbafffff] for [bus ba]
         acpi PNP0A08:09: ECAM at [mem 0xdb800000-0xdb9fffff] for [bus b8-b9]
         acpi PNP0A08:0a: ECAM at [mem 0xdbc00000-0xdbdfffff] for [bus bc-bd]
         acpi PNP0A08:0b: ECAM at [mem 0xdb400000-0xdb6fffff] for [bus b4-b6]

注意了，这个看起来包含了多个ECAM空间，其实地址是连续的一个配置空间，这个空间的
首地址在0xd0000000，其他的地址是以Bus ID为区分的，比如PNP0A08:01的总线是7b，它
的配置空间首地址就在0xd0000000+0x7b00000的位置上。

这样PCI/PCIe就可以实现这样一个过程：先基于实现相关的手段，给出每个根桥的ECAM地
址，和它支持的总线。这样，我们可以尝试访问这个总线下的每个设备，比如我们知道总
线是xx，那我们可以尝试访问xx:00.0，xx:01.0，xx:02.0，这样就可以知道这个总线下面
有那些设备了，如果发现这个设备是一个网桥，由于我们已经可以访问它的配置空间了，
我们可以给它指配一个Bus ID，然后对这个Bus重复上面的过程……一直迭代下去，就可以找
到每个设备，然后给它指定它的内存地址空间。每个设备就有一个唯一的访问地址了。

读者可能需要注意到，在这个过程中，我们只指配Bus ID，而设备和Function ID是物理上
决定的，当一个设备连到一条总线上，它属于哪个位置，是总线决定的。而一个设备提供
多个Function，这个Function是几号，这是设备的物理实现决定的。

我们用PCIe的一个例子看看具体的Bus ID是如何指配的。下图给出这样一个示例：

.. figure:: pci_bus_id_allocation.svg

由于PCIe是个P2P的总线，网桥内部实际是报文的重新调度，而没有原来PCI总线的意义了
，PCIe标准虚拟化地认为总线仍是存在的，网桥内部包含多个总线控制器。而控制器也是
一个设备，这个设备的上游总线称为Primary Bus，下游总线称为Secondary Bus。

这样网桥内部其实可以认为包含了一条Secondary Bus和多个总线控制器，虽然其实物理上
它只是多个端口之间的调度。这样，按前面提到的Bus ID分配策略，整个分配过程将类似
这样：

1. 扫描程序访问0:0.0，0:1.0, 0:2.0……的配置空间，看能否读到其中的Vendor寄存器，
   这样我们就可以刺探到这个设备是否真实存在。如果设备存在，而且类型是个EP，这个
   设备就找到了。如果这个设备又是一个总线控制器，我们进入总线设置的过程。
   
2. 假设这个总线控制器的BDF=0:x.0，根据约定，我们设置它的Primary是0，Secondary
   是我们要指派的总线号，我们分配1给它，我们就又得到一个总线1，我们可以用第一步
   一样的方法扫描1:0.0, 1:1.0, 1:2.0……得到这条总线上的设备或者总线控制器。

3. 等我们深度优先扫描完一条总线，我们也知道每个网桥的Sub Ordinary应该是多少了，
   把这个结果配置上，我们就得到这个根桥中所有总线对象的配置空间的地址了。之后的
   问题是怎么访问这些空间的问题而已。

配置空间的格式如下：

.. figure:: pci_config_space.svg

这里给出的是PCI的头部格式，PCEe只是扩展了长度，基本信息是一样的。总线指配完成
后，每个EP和网桥的配置都可以做了。系统软件就可以进一步读其中的BAR，Bar Address
Register，获知这个节点需要多大的内存地址空间，然后把分配给它的地址设置进去，就
可以完成地址的分配，上游网桥在这个过程中会记录每个下游端口的地址范围，保证调度
可以正常完成。

这个扫描过程通常BIOS和OS都需要做。BIOS必须做第一轮扫描和指配的，否则它无法找到
相关的设备完成启动过程。比如如果启动设备挂在PCIe总线上，BIOS不做这个扫描就无法
读取这个启动设备。但进入OS后，OS不一定会认可这个扫描结果，它可能全部或者部分重
新扫描，这都和具体的设计相关。

        | 作者评论：
        | 有一个问题，可能是皇帝的新衣，大家都看得见，但不一定有人说。
        | PCI/PCIe标准支持热插拔，
        | 但在标准中，无论是PCI还是PCIe标准都没有提到如果热插拔了，
        | 上面这个扫描过程怎么留下足够地址空间给多出来的设备，
        | PCI/PCIe标准只是讨论在物理上如何保证信号一致性，
        | 至于设备加进来后没法分配总线和BAR空间，这是个撞大运的事情。
        | 这种问题其实常常也是工程的常态。

要注意的是，地址请求不但可以从PCIe空间之外的其他设备发起，也可以从PCIe空间之内
的设备发起。换句话说，网桥中的EP可以直接读取其他网桥中的EP的地址空间的。这在很
多时候可以提高通讯效率，而不需要每次经过RC。不是每种服务器的实现都可以做到跨越
根桥仍可以正常或者高速访问另一个根桥树中的节点，但鲲鹏920的总线接口天然可以高带
宽直接传输这种跨根桥的消息，这可以为各种跳过CPU的通讯带来便利。比如一些集成达芬
奇AI核的解决方案中，视频设备可以直接和AI引擎接口，实现大带宽的视频识别。


鲲鹏920虚拟PCIe接口
-------------------

鲲鹏的920的每个SICL中包含一个PCI_ICL提供PCIe控制器，同时在每个主要的片上设备中
嵌入PEH和PBU提供PCIe的会话层响应，从总线访问接口看来，这些设备都连在同一个RC上，
构成一个统一的PCIe系统。

        | PEH
        | PCI Endpoint Header，是嵌入到每个鲲鹏920内部设备上的PCI EP模拟单元

        | PBU
        | PCI Bridge Unit，是嵌入到鲲鹏920内部设备中用于模拟PCI网桥的单元

这个原理图示如下：

.. figure:: kp920_pci_vpci_arch.svg

整个系统构成一个单一RC的结构，总线的DAW机制提供第一级的地址控制行为。BIOS配置
DAW把各个根桥的地址分配到不同的Station上，每个Station对应不同的根桥，通过DAW为
每个根桥分配地址，所有分配的地址共同构成一个统一的ECAM空间。

我们用给一个泰山服务器的实例来具体看这个设计。比如，下面是一个插了外置设备的泰
山服务器的lspci结果，它包含内置和外插的PCIe设备：::

        00:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        00:08.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        00:0c.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        00:10.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        00:11.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        00:12.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        01:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
        01:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)
        02:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID Tri-Mode SAS3508 (rev 01)
        03:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
        03:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
        03:00.2 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
        03:00.3 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)
        04:00.0 Signal processing controller: Huawei Technologies Co., Ltd. iBMA Virtual Network Adapter (rev 01)
        05:00.0 VGA compatible controller: Huawei Technologies Co., Ltd. Hi1710 [iBMC Intelligent Management system chip w/VGA support] (rev 01)
        74:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        74:01.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        74:02.0 Serial Attached SCSI controller: Huawei Technologies Co., Ltd. HiSilicon SAS 3.0 HBA (rev 21)
        74:03.0 SATA controller: Huawei Technologies Co., Ltd. HiSilicon AHCI HBA (rev 21)
        74:04.0 Serial Attached SCSI controller: Huawei Technologies Co., Ltd. HiSilicon SAS 3.0 HBA (rev 21)
        75:00.0 Processing accelerators: Huawei Technologies Co., Ltd. HiSilicon ZIP Engine (rev 21)
        76:00.0 Network and computing encryption device: Huawei Technologies Co., Ltd. HiSilicon SEC Engine (rev 21)
        78:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        78:01.0 RAID bus controller: Huawei Technologies Co., Ltd. HiSilicon RDE Engine (rev 21)
        79:00.0 Network and computing encryption device: Huawei Technologies Co., Ltd. HiSilicon HPRE Engine (rev 21)
        7a:00.0 USB controller: Huawei Technologies Co., Ltd. Device a23b (rev 21)
        7a:01.0 USB controller: Huawei Technologies Co., Ltd. HiSilicon USB 2.0 2-port Host Controller (rev 21)
        7a:02.0 USB controller: Huawei Technologies Co., Ltd. HiSilicon USB 3.0 Host Controller (rev 21)
        7b:00.0 System peripheral: Huawei Technologies Co., Ltd. HiSilicon Embedded DMA Engine (rev 21)
        7c:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        7d:00.0 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE RDMA Network Controller (rev 21)
        7d:00.1 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE Network Controller (rev 21)
        7d:00.2 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE RDMA Network Controller (rev 21)
        7d:00.3 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE Network Controller (rev 21)
        80:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        80:04.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        80:08.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        80:0c.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        80:10.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCIe Root Port with Gen4 (rev 21)
        85:00.0 PCI bridge: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Bridge (rev 45)
        86:00.0 PCI bridge: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Bridge (rev 45)
        86:01.0 PCI bridge: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Bridge (rev 45)
        86:02.0 PCI bridge: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Bridge (rev 45)
        86:03.0 PCI bridge: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Bridge (rev 45)
        87:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
        88:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
        89:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
        8a:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
        b4:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        b4:01.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        b4:02.0 Serial Attached SCSI controller: Huawei Technologies Co., Ltd. HiSilicon SAS 3.0 HBA (rev 21)
        b4:03.0 SATA controller: Huawei Technologies Co., Ltd. HiSilicon AHCI HBA (rev 21)
        b4:04.0 Serial Attached SCSI controller: Huawei Technologies Co., Ltd. HiSilicon SAS 3.0 HBA (rev 21)
        b5:00.0 Processing accelerators: Huawei Technologies Co., Ltd. HiSilicon ZIP Engine (rev 21)
        b6:00.0 Network and computing encryption device: Huawei Technologies Co., Ltd. HiSilicon SEC Engine (rev 21)
        b8:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        b8:01.0 RAID bus controller: Huawei Technologies Co., Ltd. HiSilicon RDE Engine (rev 21)
        b9:00.0 Network and computing encryption device: Huawei Technologies Co., Ltd. HiSilicon HPRE Engine (rev 21)
        bb:00.0 System peripheral: Huawei Technologies Co., Ltd. HiSilicon Embedded DMA Engine (rev 21)
        bc:00.0 PCI bridge: Huawei Technologies Co., Ltd. HiSilicon PCI-PCI Bridge (rev 20)
        bd:00.0 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE RDMA Network Controller (rev 21)
        bd:00.1 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE Network Controller (rev 21)
        bd:00.2 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE RDMA Network Controller (rev 21)
        bd:00.3 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE Network Controller (rev 21)

把它绘制成一个完整的连接图，就是这样的：

.. figure:: taishan920_bus_id_example.svg

两个SICL构成两个独立的NUMA域，每个NUMA域包含一个真实的PCIE根桥，和一组虚拟根桥，
每个根桥后面是一个独立的ICL的片内设备。这样，从软件上看，所有外设都是通过PCIe
连接到系统中的，但内部设备有更大的带宽和更直接的连接距离。


中断处理
---------

PCIe兼容PCI，所以它支持两种中断上报的方式，一种是简单但不灵活的INTx协议，另一种
是基于消息的MSI/MSI-X的的协议。

INTx是PCI时代的遗产，PCI时代设备的中断线合并连到系统的中断控制器上。到了PCIE时
代，由于使用SerDes了，这种方案就不可行了，传统使用INTx的设备的线被转化为链路层
的消息，由根桥按线中断进行上报，在鲲鹏920中，这个由PCIe控制器报到GICD。

而支持MSI/MSIx的设备，中断就变成写物理地址的问题，这个由ITS响应。片内虚拟PCIe设
备都采用这样的方式上报中断。


IO虚拟化
--------

IO虚拟化是在IO设备上进行特殊的设计以提高IO性能的方法。以网卡为例，比如系统中有
一张物理网卡，现在创建了两个虚拟机，VM1和VM2，每个都需要一张网卡，虚拟机可以怎
么做？

不依靠IO设备的支持，我们只能在虚拟机中模拟两个设备，设备的IO空间实现为不可访问
空间。当虚拟机访问这个空间的时候，从虚拟机退出到Hypervisor（虚拟机管理程序），
在Hypervisor中仿真这个IO行为。这个仿真过程很慢，但大部分时候是可以接受的，因为
真正的IO空间访问除了配置就是Doorbell，配置在初始化的时候才会做，而Doorbell只是
一个通知，真正的发送工作是从内存中把消息发出去。所以Hypervisor需要在网络协议栈
中仿真另一个设备（比如Linux中可以用veth仿真一个虚拟的Ethernet设备），模拟从外部
收到一个报文，然后经过Linux内核的调度过程，再发送到本地的应用上，或者转发到真实
的那个物理网口上，这个地方经过多次内存拷贝，也会降低系统的性能。

IO虚拟化的意思是：真实的物理设备的IO空间分成多份，给每个虚拟机一份，这样
Hypervisor就不需要仿真IO行为了，物理硬件收到这个请求，直接根据要求，直接发出去
或者路由回本地就可以了，整个过程都不需要软件参与。

        .. figure:: vio_vs_novio.svg

可以看到IO虚拟化的要求其实就是要求硬件一开始准备多份自己的IO空间，直接把每个IO
空间作为一个虚拟设备进行处理。

但仅这样理解也是不够的，因为其实用一个真实设备的IO空间是不能作为虚拟IO设备来用
的。这个区别在于，当VM的驱动向IO空间中写入一个地址，这个地址并非是一个物理地址，
而是Hypervisor给VM生成的“中间物理地址”（下面简称IPA）。所以，虚拟设备不但要给VM
一个真实的IO空间，还需要可以解释IPA。这就是IO虚拟化的主要内涵了。

每种硬件可以有自己的独特IO虚拟化方案。PCIE把这个功能标准化，定义为一个设备特性
，IO Virtualization。它有两个标准，Multi-Root IOV和Single-Root IOV，后者是前者
的子集，但实际使用中大部分平台都只实现后者，所以本文主要讨论的也是后者。

这里所谓Multi-root和Single Root主要是说有多少个RC，Root Complex。Root Complex在
PCI/PCIE标准中是个实现相关的虚拟概念，用于指代一个独立管理整个PCI/PCIE总线的对
象，比如一个操作系统和它管理之下的整个计算子系统。对于大部分服务器，包括泰山服
务器，我们也只有一个这样的对象。所以，大部分时候我们谈PCIE的IOV功能，都是指
SR-IOV。

SR-IOV实现为一个PCIE的Capability。我们先解释一下PCIE的Capability的概念是什么。

Capability的概念从PCI标准继承。这是个很简单直接的设计：PCI的配置空间定义了一个
标准的数据结构，定义了基本的访问接口，比如Device ID，Vendor ID，BAR空间，等等。
但如果要增加新的可选功能，就需要有一个动态的方法可以在这个标准的头部后面增加新
的数据结构。PCI在标准头上放了一个指针：Capabilties Point，指向下一个Capability
的配置结构，这些Capability配置结构每个有相同的头，给出Capability的ID和下一个
Capability的指针，这样所有的Capability就构成一条单链表。顺着链表就可以枚举所有
的Capability，然后根据那种Capability的具体定义进行配置就可以了。

Linux下lspci -vvv命令可以输出每个设备的Capabilities。比如下面是一个鲲鹏920内置
SAS控制器的Capabilties：::

        74:02.0 Serial Attached SCSI controller: Huawei Technologies Co., Ltd. HiSilicon SAS 3.0 HBA (rev 21)
                Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
                Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
                Latency: 0
                Interrupt: pin ? routed to IRQ 1360
                NUMA node: 0
                Region 5: Memory at a2000000 (32-bit, non-prefetchable) [size=32K]
                Capabilities: [40] Express (v2) Root Complex Integrated Endpoint, MSI 00
                        DevCap:	MaxPayload 128 bytes, PhantFunc 0
                                ExtTag+ RBE+
                        DevCtl:	Report errors: Correctable- Non-Fatal- Fatal- Unsupported-
                                RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop-
                                MaxPayload 128 bytes, MaxReadReq 512 bytes
                        DevSta:	CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend-
                        DevCap2: Completion Timeout: Not Supported, TimeoutDis+, LTR-, OBFF Not Supported
                        DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled
                Capabilities: [80] MSI: Enable+ Count=32/32 Maskable+ 64bit+
                        Address: 00000000fdfff040  Data: 0000
                        Masking: 0000f7f9  Pending: 00000000
                Capabilities: [b0] Power Management version 3
                        Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
                        Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME-
                Kernel driver in use: hisi_sas_v3_hw
                Kernel modules: hisi_sas_v3_hw

SR-IOV也是一种Capability，鲲鹏920的网卡系统就支持这个Capability，我们同样可以通
过lspci看到它：::

	Capabilities: [200 v1] Single Root I/O Virtualization (SR-IOV)
		IOVCap:	Migration-, Interrupt Message Number: 000
		IOVCtl:	Enable- Migration- Interrupt- MSE- ARIHierarchy-
		IOVSta:	Migration-
		Initial VFs: 3, Total VFs: 3, Number of VFs: 0, Function Dependency Link: 03
		VF offset: 14, stride: 1, Device ID: a22e
		Supported Page Size: 00000553, System Page Size: 00000001
		Region 0: Memory at 00002001210d0000 (64-bit, prefetchable)
		Region 2: Memory at 0000200120d00000 (64-bit, prefetchable)
		VF Migration: offset: 00000000, BIR: 0

它的配置空间如下：

.. figure:: sr-iov_config_space.svg

在SR-IOV的定义中，原来的Function称为Physical Function，PF，而创建的虚拟功能称为
Virtual Function， VF。使用者通过配置PF的SR-IOV Capabilty的配置空间创建新的VF。

其中，这里的Total VFs给出总共支持多少个VF。使用者可以通过写入不超过Total VFs的
数值到Num of VFs域中创建VF。新的VF会获得自己的BDF。PF的硬件基于这个范围的BDF
处理对应VF的配置请求即可。

上面的逻辑解决了IO空间怎么分配的问题，但没有讨论IPA怎么翻译的问题。地址翻译是
计算子系统的工作，PCIE总线本身不能解决这个问题，所以，它把这个问题推给RC。下图
是一个示意：

.. figure:: pcie_ats_atc.svg

Address Translation Service，ATS是一个独立的服务，EP认为RC中存在这个服务，支持
虚拟化的设备通过PCIe的链路层消息向这个服务请求地址翻译，翻译后会得到物理地址和
一个Tag，之后地址请求就以这个物理地址和Tag为基础发出，RC根据物理地址和Tag决定是
否允许这个访问过程。

PCIe允许EP中缓存这个被翻译的地址，这个缓存服务称为ATC，Address Translation
Client，它需要响应RC的Cache Invalidate等命令。

我们要注意到，ATS是一个服务，而不是地址访问代理，所以实际上一个非法的EP完全可以
直接发出一个物理地址，说这是翻译的结果。

所以，其实这个协议并不合理，我们几乎没有见过有实际使用的例子。在鲲鹏920中，实际
是从EP上发出虚拟地址，在进入DAW前，由SMMU单元进行地址翻译，最终成为物理地址。

鲲鹏920的SMMU单元遵循ARM SMMUv3标准，它可以基于Stream ID和Substream ID指定一个
地址，Stream ID对应BDF，而Substream ID对应ASID。在PCIe地址请求中包含这两个信息
，SMMU中就可以定位到对应的页表，实现地址翻译。所以Hypervisor只要在SMMU中指定好
具体的页表就可以了。ARM的SMMU页表格式和MMU的页表格式兼容，这很容易实现进程和设
备共享同一个地址空间。


Linux下使用虚拟设备
````````````````````
Linux的SR-IOV接口更新速度相当快，很多设备驱动没有跟随升级，所以不是每种设备的用
法相同的。鲲鹏920至少到Linux 5.6都是直接支持最新的用法的，本小结我们看看这个工
作逻辑是什么样的。

我们用鲲鹏内置网卡作为例子来研究这个过程。

首先，Linux PCI子系统进行设备发现，会找到每个PF，比如这个：::

        bd:00.0 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE RDMA Network Controller (rev 21)
        bd:00.1 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE Network Controller (rev 21)
        bd:00.2 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE RDMA Network Controller (rev 21)
        bd:00.3 Ethernet controller: Huawei Technologies Co., Ltd. HNS GE/10GE/25GE Network Controller (rev 21)

我们可以lspci直接从配置空间里看到这个设备是否支持SR-IOV。当然，作为操作系统的用
户我们应该从操作系统的接口上看，我们可以通过/sys/bus/pci/devices里面找到这四个
设备：::

        lrwxrwxrwx 1 root root 0 Mar 12 01:08 0000:bd:00.0 -> ../../../devices/pci0000:bc/0000:bc:00.0/0000:bd:00.0
        lrwxrwxrwx 1 root root 0 Mar 12 01:08 0000:bd:00.1 -> ../../../devices/pci0000:bc/0000:bc:00.0/0000:bd:00.1
        lrwxrwxrwx 1 root root 0 Mar 12 01:08 0000:bd:00.2 -> ../../../devices/pci0000:bc/0000:bc:00.0/0000:bd:00.2
        lrwxrwxrwx 1 root root 0 Mar 12 01:08 0000:bd:00.3 -> ../../../devices/pci0000:bc/0000:bc:00.0/0000:bd:00.3

我们拿最后一个设备bd:00.3作为研究对象，查看它的内容：::

        root@host:/sys/bus/pci/devices/0000:bd:00.3# ls -l sriov_*
        -rw-rw-r-- 1 root root 4096 Mar 12 02:25 sriov_drivers_autoprobe
        -rw-rw-r-- 1 root root 4096 Mar 12 02:25 sriov_numvfs
        -r--r--r-- 1 root root 4096 Mar 12 02:25 sriov_offset
        -r--r--r-- 1 root root 4096 Mar 12 02:25 sriov_stride
        -r--r--r-- 1 root root 4096 Mar 12 02:25 sriov_totalvfs
        -r--r--r-- 1 root root 4096 Mar 12 02:25 sriov_vf_device

其中的sriov_开头的文件就是操作系统暴露的SR-IOV相关控制属性，我们先看sriov_totalvfs：::

        root@host:/sys/bus/pci/devices/0000:bd:00.3# cat sriov_totalvfs 
        3
        root@host:/sys/bus/pci/devices/0000:bd:00.3# cat sriov_numvfs
        0

这个设备支持最多3个VF，现在的配置是0。我们把3个都创建了：::

        root@host:/sys/bus/pci/devices/0000:bd:00.3# echo 3 > sriov_numvfs
        root@host:/sys/bus/pci/devices/0000:bd:00.3# ls -l
        ...
        lrwxrwxrwx 1 root root       0 Mar 12 02:29 virtfn0 -> ../0000:bd:02.1
        lrwxrwxrwx 1 root root       0 Mar 12 02:29 virtfn1 -> ../0000:bd:02.2
        lrwxrwxrwx 1 root root       0 Mar 12 02:29 virtfn2 -> ../0000:bd:02.3

可以看到，PF的bdf是bd:00.3，创建的虚拟设备的BDF是bd:02.1,2,3。这里分配了新的
device id，并复用这个id创建了三个Function。用lspci看virtfn0的配置是这样的：::

        bd:02.1 Ethernet controller: Huawei Technologies Co., Ltd. HNS Network Controller (Virtual Function) (rev 21)
                Subsystem: Huawei Technologies Co., Ltd. Device 0000
                Control: I/O- Mem- BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
                Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
                Latency: 0
                NUMA node: 2
                Region 0: [virtual] Memory at 2001210d0000 (64-bit, prefetchable) [size=64K]
                Region 2: [virtual] Memory at 200120d00000 (64-bit, prefetchable) [size=1M]
                Capabilities: [40] Express (v2) Endpoint, MSI 00
                        DevCap:	MaxPayload 128 bytes, PhantFunc 0, Latency L0s <64ns, L1 <1us
                                ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ SlotPowerLimit 0.000W
                        DevCtl:	Report errors: Correctable- Non-Fatal- Fatal- Unsupported-
                                RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop- FLReset-
                                MaxPayload 128 bytes, MaxReadReq 128 bytes
                        DevSta:	CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend-
                        LnkCap:	Port #0, Speed 2.5GT/s, Width x1, ASPM not supported, Exit Latency L0s <64ns, L1 <1us
                                ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                        LnkCtl:	ASPM Disabled; RCB 64 bytes Disabled- CommClk-
                                ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                        LnkSta:	Speed unknown, Width x0, TrErr- Train- SlotClk- DLActive- BWMgmt- ABWMgmt-
                        DevCap2: Completion Timeout: Not Supported, TimeoutDis+, LTR-, OBFF Not Supported
                        DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled
                        LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete-, EqualizationPhase1-
                                 EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest-
                Capabilities: [a0] MSI-X: Enable+ Count=67 Masked-
                        Vector table: BAR=0 offset=00000000
                        PBA: BAR=0 offset=00008000
                Capabilities: [b0] Power Management version 3
                        Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
                        Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME-
                Capabilities: [100 v1] Access Control Services
                        ACSCap:	SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-
                        ACSCtl:	SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-
                Capabilities: [300 v1] Transaction Processing Hints
                        Device specific mode supported
                        No steering table available
                Capabilities: [450 v1] Alternative Routing-ID Interpretation (ARI)
                        ARICap:	MFVC- ACS-, Next Function: 0
                        ARICtl:	MFVC- ACS-, Function Group: 0
                Kernel driver in use: hns3
                Kernel modules: hclgevf, hns3

这个信息显示，这个设备已经绑定了驱动hns3，所以其实这个设备已经可以用了，用
ifconfig或者ip命令都可以看到它：::

	enp189s0f3: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
		ether 44:00:4d:a7:95:43  txqueuelen 1000  (Ethernet)
		RX packets 0  bytes 0 (0.0 B)
		RX errors 0  dropped 0  overruns 0  frame 0
		TX packets 0  bytes 0 (0.0 B)
		TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

	enp189s2f1: flags=4098<BROADCAST,MULTICAST>  mtu 1500
		ether 5a:21:6d:a7:84:1d  txqueuelen 1000  (Ethernet)
		RX packets 0  bytes 0 (0.0 B)
		RX errors 0  dropped 0  overruns 0  frame 0
		TX packets 0  bytes 0 (0.0 B)
		TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

	enp189s2f2: flags=4098<BROADCAST,MULTICAST>  mtu 1500
		ether c2:a1:d0:1f:8f:7b  txqueuelen 1000  (Ethernet)
		RX packets 0  bytes 0 (0.0 B)
		RX errors 0  dropped 0  overruns 0  frame 0
		TX packets 0  bytes 0 (0.0 B)
		TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

如果读者不能确定这些就是增加的链路，可以用ethtool -i去确认：::

	# ethtool -i enp189s2f1
	driver: hns3
	version: 5.0.0-23-generic SMP mod_unload
	firmware-version: 0x01091d10
	expansion-rom-version:
	bus-info: 0000:bd:02.1
	supports-statistics: yes
	supports-test: no
	supports-eeprom-access: no
	supports-register-dump: yes
	supports-priv-flags: no

这和一个真实的网卡没有什么区别，用户直接给它配置地址也可以当做一个真正的网卡来
用，只是它对外并没有真实的物理链路而已。

现在我们看怎么把这个网卡用于虚拟机。首先我们不能让本地系统直接使用它，所以我们
首先把它的驱动绑定去掉：

我们先进入它的驱动目录，然后把设备名称写入unbind：::

        cd /sys/devices/pci0000:bc/0000:bc:00.0/0000:bd:00.3/virtfn0/driver
        echo "0000:bd:00.3" > unbind

/sys/devices/pci0000:bc/0000:bc:00.0/0000:bd:00.3/virtfn0中driver这个链接就会消
失。virtfn0仍是一个pci设备，但没有pci驱动绑定它。

然后给它绑定vfio-pci驱动，这个驱动把任何pci设备暴露为一个vfio接口，供虚拟机使用
。为了做到这一点，我们需要做三个事情：

* 保证vfio-pci驱动在内核中
* 设置这个设备的driver_override为vfio-pci，要求它不要绑定默认的驱动
* 要求pci总线对这个设备重新probe

::
        modprobe vfio-pci
        cd /sys/devices/pci0000:bc/0000:bc:00.0/0000:bd:00.3/virtfn0
        echo "vfio-pci" > driver_override
        echo "0000:bd:02.1" > /sys/bus/pci/drivers_probe

最后我们把这个设备“种入”虚拟机中：::

        qemu-system-aarch64 -machine virt,kernel_irqchip=on,gic-version=3 \
                -cpu host -enable-kvm -smp 1 \
                -nographic -machine virt \
                -m 1024m -kernel ~/work/mainline-kernel/arch/arm64/boot/Image \
                -drive if=none,file=../ubuntu-14.04-server-cloudimg-arm64-uefi1.img,id=hd0 \
                -device virtio-blk-device,drive=hd0 \
                -append "root=/dev/vda1 init=/bin/sh console=ttyAMA0 earlycon=pl011,0x90000000" \
                -device vfio-pci,host=0000:bd:02.1,id=net0,bus=pcie.0,addr=0x8 \

前面的参数和这里的讨论无关，我们留给讨论计算子系统的时候再来讨论，这里仅讨论最
后一行。这里-device为系统定义了一个设备，设备类型是vfio-pci，这是要求qemu本身
用vfio-pci驱动的接口去访问本地的设备设备。host给出本地的设备名称，这里是
0000:bd:02.1。id给出设备名称，bus给出虚拟PCIE总线的总线编号，这里用了pcie.0，
表示这是连在虚拟根桥上的设备，最后addr是根桥设备上的设备号。

进入虚拟机后，我们再用lspci来看虚拟机里面的PCI总线接口，就是这样的：::

        root@(none):/sys/bus/pci/devices# lspci
        00:00.0 Host bridge: Red Hat, Inc. Device 0008
        00:01.0 Ethernet controller: Red Hat, Inc Virtio network device
        00:08.0 Ethernet controller: Device 19e5:a22e (rev 21)

        root@(none):/sys/bus/pci/devices# lspci -t
        -[0000:00]-+-00.0
                   +-01.0
                   \-08.0

00.0和01.0是虚拟机本身创建的节点，08是我们前面指定的设备地址，这个设备的配置空间
被直接暴露给了虚拟机，我们可以对照它的配置空间信息：::

	00:08.0 Ethernet controller: Device 19e5:a22e (rev 21)
		Subsystem: Device 19e5:0000
		Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
		Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
		Region 0: Memory at 8000100000 (64-bit, prefetchable) [disabled] [size=64K]
		Region 2: Memory at 8000000000 (64-bit, prefetchable) [disabled] [size=1M]
		Capabilities: [40] Express (v2) Root Complex Integrated Endpoint, MSI 00
			DevCap: MaxPayload 128 bytes, PhantFunc 0
				ExtTag+ RBE+
			DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported-
				RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop-
				MaxPayload 128 bytes, MaxReadReq 128 bytes
			DevSta: CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend-
			DevCap2: Completion Timeout: Not Supported, TimeoutDis+, LTR-, OBFF Not Supported
			DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled
		Capabilities: [a0] MSI-X: Enable- Count=67 Masked-
			Vector table: BAR=0 offset=00000000
			PBA: BAR=0 offset=00008000
		Capabilities: [b0] Power Management version 3
			Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
			Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME-
		Capabilities: [100 v1] Access Control Services
			ACSCap: SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-
			ACSCtl: SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-
		Capabilities: [300 v1] Transaction Processing Hints
			Device specific mode supported
			No steering table available

除了动态Probe的内容，大部分的设置都是一样的。

在前面例子中，也许读者们会注意到一个细节：这里每个鲲鹏网卡的PF只支持3个VF。这在
很多场合（比如IaaS）不够用。这其实是个设置问题：我们可以选择更多的VF，每个VF有
更少的队列，也可以选择更多的VF，每个VF有更少的队列。这种类型的配置PCIe层面不支
持，鲲鹏通过Boot Loader（或者一般PC用户理解的BIOS）对此进行选择。说到底，要支持
多少VF，是以硬件能力决定的：

.. figure:: kp920_hns3_boot_config.png
