.. Copyright by Kenneth Lee. 2020. All Right Reserved.

内存访问模型
============

内存访问的的原子化行为
----------------------

一条独立的指令，从写指令的人看来，这是原子的，但在CPU执行的时候，呈现为一组复
杂的微架构行为，所以，当我们从CPU上发出一条指令的时候，程序员不能简单假设这个
指令从其他CPU、加速器或者其他设备上看到也是一样的。

本小节我们看看鲲鹏上（就这个问题来说，其实主要是ARMv8的架构定义上，但鲲鹏920芯
片内部不但包含有CPU，所以不见得会完全重合，但由于其他处理单元也需要维持CPU的语
义，所以基本上是重合的）是如何解决这个问题的。

作为RISC指令集，ARMv8的访存指令和计算指令是明确分开的。程序员很容易区分这些指
令的行为，这些指令要不从内存中读写数据，要不基于寄存器的数据进行内部的运算。不
会出现一边计算，一边访问内存的行为。

        | RISC
        | Reduced Instruction Set Computer，精简指令集计算机。
        | 这是一种通用CPU指令设计策略，原始的含义是简化指令集的功能。
        | 让每个指令完成一个独立的功能，而不会产生执行一个指令，
        | CPU内部要反复调度多个硬件多步完成这个功能的情况。
        | 这种设计对于流水线布置，性能优化，都有更好的预期性，
        | 已经呈现现代CPU设计的主流，即使在部分表面指令呈现为非RISC
        | 行为的处理器中，内部实现也是按RISC理念设计的，只是把一个
        | 复杂的指令请求翻译为多个RISC形式的指令发送而已。

正如我们在介绍泰山核的内部结构时提到的那样。在执行这些指令的时候，CPU可能会进
行预测执行，访问内存会产生Cache合并，这个优化在每代的硬件中都会进行优化。所以
对于软件工程师，我们其实更关心的是底层微架构在指令上的“承诺”。我们不能基于硬件
做成什么样来编程。而对于鲲鹏920，这个承诺就是ARMv8构架定义。

在ARMv8的架构定义中，所有单寄存器，而且和寄存器大小对齐的地址访问指令，都保证
是原子的。换句话说，你在一个CPU里发出这样的指令去修改内存，其他的CPU或者加速器
，必然或者看到这个指令生效前的情况，或者看到生效后的情况，不会出现修改了一半的
情况。

这一点保证了多个CPU同时访问内存的时候结果是有办法预期的，否则多核通讯就没法直
接基于内存做了。

其他每个具体指令的内存行为ARMv8架构设计中有具体的定义，我们这里不是要翻译ARMv8
架构手册，所以我们这里不深入进去。

我们看看其他的原子需求在这个平台是怎么实现的。首先是CAS。CAS是Compare-And-Set
的简称。这是软件实现锁的的基本方法：在内存上放一个变量v，每个CPU不断去读这个变
量，如果它是a的时候，表示某种资源（比如其他内存变量），立即把它设置为b，使用那
种资源，用完后，再把它设置为a，让其他CPU可以使用它。

这个逻辑很简单，但需要保证读到它是a和把它设置为b这两个动作是原子的，否则这个逻
辑就不成立了。完成这样原子指令就称为CAS。

ARMv8定义了一组包括CAS指令在内的原子指令：

* CAS Compare-And-Set，有不同字长的版本（下同）
* LDADD/STADD Load/Store-and-Add
* LDCLR/STCLR Load/Store-and-Clear
* LDEOR/STEOR Load/Store-and-EclusiveOR
* LDSET/STSET Load/Store-and-Set
* LDMAX/STMAX Load/Store-and-取最大值，分不同字长和有符号数和无符号数版本（下同）
* LDMIN/STMIN Load/Store-and-取最小值

所有这些指令都是在内存操作上叠加一个运算以实现多核之间数据更新，这说起来违背我
们前面提到的RISC设计原则，但这就是这个产业的特点，永远都是个度的问题。

原子指令在软件上看来逻辑并不复杂，但在微架构上看，成本是很高的。如果我们把CPU
和内存都看做是总线上的一个个独立的实体，有一个CPU要做CAS指令，这个CPU需要先从
内存中读一个值，同时要在内存控制器上设置一个标志，保证其他CPU写不进去，等它比
较完了，然后再决定写一个值回去，才会让其他CPU写入。

不同微架构实现有不同方法对行为进行优化，在鲲鹏920上，原子指令的请求需要在
L3Cache上进行排队，保证在原子操作的多个动作之间能维持原子指令要求的语义。这个
排队本身也有成本。所以没有原子需要就不要轻易用原子变量，这其实是有成本的。

        .. figure:: atomic_inst_arch.svg

原子指令在遇到冲突的时候，会直接阻住CPU的执行。这个对于很多CPU的锁行为是有害的
，所以ARMv8还有一套exclusive指令：

* LDA Load-Acquire，有不同字长的版本（下同）
* STL Store-Release

这个方案把部分自由度还给软件：LDA读一个内存，同时在这个内存地址上打上一个标记
，STL负责对这个地址写入一个值，如果这这种有其他CPU修改过这个值，这个写入会失败
本CPU可以重新做这个LDA。这个方案不需要CPU作等待，CPU如果写不成功，可以先去做别
的操作，根据策略重试。这对于共享的核的数量非常多的时候，收益会很大。

和原子指令把同步行为推到L3 Cache不同，在鲲鹏920上Exclusive指令的同步是在靠近
CPU的Cache上做的，LDA在本地的Cacheline上设置了一个标记，其他CPU如果更新了这个
数据，会把这个Cacheline置为无效，这时再做STL就可以知道数据更改过了。但Cache同
步本身也有成本（共享这个数据的CPU越多成本越高），所以这也是个权衡的问题，通常
我们用后者做锁操作，而用前者做原子变量一类的功能。

        .. figure:: exclusive_inst_arch.svg

一般应用的工程，一般比较好的实践是用对应语言的一般语义编程比较好，把这个优化工
作留给芯片和底层软件的优化者是个更好的配合策略。


内存Cache和Cache Coherency设计
------------------------------

相对CPU的计算单位，一次时钟跳变，或者一般说一个Cycle，内存的速度非常慢，在配置
xxxx主频的DDR4上，一个内存访问操作需要xxx个Cycle（待确定，但一般我会认为是200
个Cycle左右）。

这时就会选择使用Cache。Cache是更快、更小、更靠近CPU的存储单元。Cache的存在和设
计上不断的改进和变化，控制要素都是它的这三个特征：

* 更快：更快而没有取代DDR内存，显而易见，因为成本高
* 更小：同上
* 更靠近CPU：即使没有前面这个成本的问题，大量的CPU需要访问相同的地址，也需要排
  队，Cache仍可以带来优势。

        | 根据 [1]_ 的数据，2012年SRAM的单次访问时间是0.5-2.5ns，
        | 每GB的价格是500-1000美元。而DDR的单次访问时间是60-70ns，
        | 每GB的架构是10-20美元。

.. [1] Computer Organization and Design: RISC-V Edition

鲲鹏920的Cache分近CPU Cache(L1和L2）和远CPU Cache（L3），示意如下图：

        .. figure:: kp920_cache_arch.svg

近CPU Cache可以不跨域系统总线访问，远CPU Cache则需要跨越总线。后者被分成两部分
。Cache内容索引（Tag）在CPU和加速器一侧，而实际的数据在总线的另一端。


Cacheline和关联性
`````````````````
Cache比目标空间小，这就造成一个目标地址标识的问题的了。如果一个字长的数据需要
一个地址，那么一个64位的数据就需要一个64位的索引（Tag），用更大的数据粒度，对
地址的要求就可以下降。这个粒度就称为Cachenline。不同层级的Cacheline的长度不需
要完全一致，鲲鹏920的Cachenline长度，可以在Linux发行版上运行getconf命令获得。
但在本文写作的时候，这里只有L1 Cache的长度，这是一个Bug。

读者如果需要参考数据，鲲鹏920的L1和L2 Cacheline长度是64个字节，而L3 Cachenline
的长度是128个字节。这些值的选择，都是针对应用的权衡的结果，并没有确定的规矩。

关联性则是进一步降低对Tag的要求：如果规定每个Cacheling只能对应一定数量的地址，
Tag的大小可以进一步降低：

        .. figure:: cache_addressing.svg

ICache和DCache
``````````````
和很多其他CPU的选择一样，鲲鹏的指令和数据Cache（分别称为ICache和DCache）是分离
的。对于冯诺依曼计算机来说，指令和数据其实都是数据。所以很多时候把两者分离是没
有必要的，但在最靠近CPU一级，把两者分离有好处：它们的预取策略，Tag方式是不同的
。

        | 冯诺依曼计算机
        | todo

比如，一般数据的Cache地址是基于物理地址的（相同的物理地址可以被不同的虚拟地址
共享），而ICache基于虚拟地址没有好处，因为ICache在解码的时候并不知道物理地址，
物理地址需要经过MMU进行翻译才能知道。

鲲鹏920 ICache声称是基于虚拟地址的，这可以从Linux内核的启动日志中可以看出来：
        ::

        Detected VIPT I-cache on CPU0
        Detected VIPT I-cache on CPU1
        Detected VIPT I-cache on CPU2
        Detected VIPT I-cache on CPU3
        ...

这里声称每个CPU是的ICache是VIPT的，意味着Linux会以虚拟地址为考量进行相应Cache
的刷新，但这其实并非是鲲鹏920 ICache设计的全部，因为其实鲲鹏920的取指部件是同
时发出物理地址和虚拟地址两个请求的，根据哪个先返回决定用那个地址。但对操作系统
暴露的接口，它是认为它是VIPT的。

        | VIPT
        | Virtual Index Physical Tag，表示用虚拟地址做索引，
        | 而用物理地址做Tag匹配的Cache Tag设计方式。
        | 与之对应的还有VIVT，PIPT等其他策略。

我们说明这一点，是要让读者看到，其实大部分时候没有必要靠软件去响应硬件的多种变
化，因为硬件也在玩小聪明，软件也在玩小聪明，系统就不再聪明了。所以不如简单一点
严格按接口和自己的目标来程序。

地址交织
````````
地址交织也是一种提升访问带宽的技术。比如一个系统有两个内存单元（比如两条DIMM条
），那么我们可以有两种常见的编址方法。一种是直接分段使用，比如[0,X)是第一片内存
的空间，[X, Y]是第二片内存的空间。这种如果你连续访问一片内存，比如做一个memcpy
，这个访问的带宽受限于到其中一个内存单元的带宽。

第二种方法是第一个地址放在第一片内存中，第二个地址放在第二片内存中，第三个地址
放在第一片内存中，第四个地址在第二片内存中。这样在做连续地址访问的时候内存的带
宽就可以扩展一倍。这种编址的方法就叫地址交织。

地址交织可以发生在Cache上，也可以发生的内存中，都是总线地址访问策略，在鲲鹏920
中，用户可以通过BIOS配置设置不同的交织方法。但其实并没有策略，每种交织策略只是
对某种应用内存访问模式的一种妥协。

鲲鹏920支持如下内存映射模式：

* Rank-Row-Bank-Col
* Rank-Row-Col-Bank-Col
* Rank-Row-Rank-Col-Bank-Col
* Rank-Row-Bank-Row-Bank-Raw
* Rank-Row-Col-Row-Bank-Col

它们分别根据内存条的不同Rank，Row, Bank和Columne进行编址，其中最前面RRBC模式就
是没有交织的一般方式。


Cache预取
`````````
访问内存比较慢，访问Cache就快得多，越靠近CPU的Cache访问越快。

todo：提供一个示例程序，和运行结果。

这样很自然我们我想到要打这个时间差，比如我们可以先发起一个Cache访问操作，然后
先完成其他计算，然后在来使用这个地址，这样内存加载和计算就可以并行起来。

比如下面是一个矩阵乘法的算法：

.. code-block:: c

        int a[W][H]; //第一个矩阵
        int b[H][W]; //第二个矩阵
        int c[W][W]; //结果

	for (i=0; i<W; i++) {
		for(j=0; j<W; j++) {
			c[i][j] = 0;
			for (k=0; k<H; k++) {
				c[i][j] += a[i][k]*b[k][j];
			}

		}
        }

这个循环中的关键计算是中间的核心乘法，但乘法的时间还不如我们把a，b，c的内容从
DDR中提取出来的时间长，而且我们在计算这个乘法的时候，我们其实还知道下一步需要
从哪里加载内存，我们就可以把这个要求提前下达了，比如这样：

.. code-block:: c

	for (i=0; i<W; i++) {
		for(j=0; j<W; j++) {
			c[i][j] = 0;
			if (!(j%INT_PER_CACHELINE))
				__builtin_prefetch((const void *)&c[i][j+INT_PER_CACHELINE], 1, 3);
			for (k=0; k<H; k++) {
				c[i][j] += a[i][k]*b[k][j];
				if (!j && !(k%INT_PER_CACHELINE))
					__builtin_prefetch((const void *)&a[i][k+INT_PER_CACHELINE], 0, 3);
				if (!i && !(j%INT_PER_CACHELINE))
					__builtin_prefetch((const void *)&b[k][j+INT_PER_CACHELINE], 0, 3);
			}

		}
        }

但实际上，如果在鲲鹏920上运行这个程序，并不会让速度更快，因为除了软件在做
prefetch外，硬件也在预判指令序列的行为，这两者一综合，这个效果就没有了。

todo：硬件预取行为（其实我觉得软件就别参合这事儿）

Cache Coherency
```````````````

Cache QoS
``````````

内存顺序模型
------------
todo: tso, twm

NUMA
----
todo

内存交织
--------
todo

.. vim: fo+=mM tw=78
